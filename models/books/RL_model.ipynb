{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65P9MsHvPKMR"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.layers.merge import Add, Multiply\n",
        "import keras.backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwKfxNfeDHhQ",
        "outputId": "9204464b-efd7-4795-d170-6f95828acf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "print(tf. __version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JJOuIQWPPTY"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "learning_rate = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_decay = .995\n",
        "gamma = .95\n",
        "tau   = .125\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwMEnI7BFlky"
      },
      "source": [
        "# Class Actor Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHB1NM7JSmUn"
      },
      "outputs": [],
      "source": [
        "class Actor_Critic_Model:\n",
        "  def __init__(self, env, sess):\n",
        "\n",
        "    self.env = env\n",
        "    self.sess = sess\n",
        "\n",
        "    self.learning_rate = 0.001; self.epsilon = 1.0\n",
        "    self.epsilon_decay = .995; self.gamma = .95; self.tau = .125\n",
        "\n",
        "\n",
        "    #Setting up the Actor Model\"\n",
        "\n",
        "    self.memory = deque(maxlen=2000)\n",
        "\n",
        "    # Actor Model State Input\n",
        "    self.actor_state_input, self.actor_model = self.create_Actor_Model()\n",
        "    _, self.target_actor_model = self.create_Actor_Model()\n",
        "\n",
        "    self.actor_critic_grad = tf.compat.v1.placeholder(tf.float32, \n",
        "    [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
        "\n",
        "    actor_model_weights = self.actor_model.trainable_weights\n",
        "    self.actor_grads = tf.gradients(self.actor_model.output, \n",
        "                                    actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
        "    grads = zip(self.actor_grads, actor_model_weights)\n",
        "    self.optimize = tf.keras.optimizers.RMSprop(self.learning_rate).apply_gradients(grads)\n",
        "\n",
        "\t  # Setting up the Critic Model\"\n",
        "    \n",
        "    self.critic_state_input, self.critic_action_input, self.critic_model = self.create_critic_model()\n",
        "    _,  _, self.target_critic_model = self.create_critic_model()\n",
        "    \n",
        "    self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) # where we calcaulte de/dC for feeding above\n",
        "    \n",
        "    # Initialize for later gradient calculations\n",
        "    self.sess.run(tf.compat.v1.initialize_all_variables())\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "  def create_Actor_Model():\n",
        "    \n",
        "    state_input = Input(shape = env.observation_space.shape)\n",
        "    h1 = Dense(24, activation = 'relu')(state_input)\n",
        "    h2 = Dense(24, activation = 'relu')(h1)\n",
        "    h3 = Dense(24, activation = 'relu')(h2)\n",
        "    output = Dense(env.action_space.shape[0], activation='relu')(h3)\n",
        "\n",
        "    actor = Model(inputs = state_input, outputs=output)\n",
        "    adam = tf.optimizers.Adam(lr = 0.001)\n",
        "    \n",
        "    actor.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "    return state_input, actor\n",
        "  \n",
        "  def create_Critic_Model():\n",
        "\n",
        "    \"\"\"\n",
        "    Critic Model takes both as input the state environment and the action space and calculate a corresponding valuation\n",
        "    We do this by a series of fully-connected layers, with a layer in the middle that merges the two before combining into the final Q-value prediction\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    state_input = Input(shape = env.observation_space.shape)\n",
        "    state_h1 = Dense(24, activation = 'relu')(state_input)\n",
        "    state_h2 = Dense(48)(state_h1)\n",
        "    \n",
        "    action_input = Input(shape = env.observation_space.shape)\n",
        "    action_h1 = Dense(48)(action_input)\n",
        "\n",
        "    critic_input    = Add()([state_h2, action_h1])\n",
        "    h1 = Dense(24, activation = 'relu')(critic_input)\n",
        "\n",
        "    output = Dense(1, activation='relu')(h1)\n",
        "    \n",
        "    critic = Model(inputs = [state_input, action_input], outputs = output)\n",
        "\n",
        "    adam = tf.optimizers.Adam(lr = 0.001)\n",
        "    \n",
        "    critic.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "    return state_input, action_input, critic\n",
        "  \n",
        "  def remember(self, cur_state, action, reward, new_state, done):\n",
        "    self.memory.append([cur_state, action, reward, new_state, done])\n",
        "    \n",
        "  def train_actor(self, samples):\n",
        "    for sample in samples:\n",
        "      \n",
        "      current_state, action, reward, new_state, _ = sample\n",
        "      predicted_action = self.actor_model.predict(current_state)\n",
        "      grads = self.sess.run(self.critic_grads, \n",
        "                            feed_dict={ self.critic_state_input:  current_state, \n",
        "                                        self.critic_action_input: predicted_action})[0]\n",
        "      self.sess.run(self.optimize, feed_dict={ self.actor_state_input: current_state, \n",
        "                                               self.actor_critic_grad: grads})\n",
        "\n",
        "\n",
        "  def train_critic(self, samples):\n",
        "\n",
        "    for sample in samples:\n",
        "      current_state, action, reward, new_state, done = sample\n",
        "      # print(current_state)\n",
        "      # print(new_state)\n",
        "\n",
        "      if not done:\n",
        "        target_action = self.target_actor_model.predict(new_state)\n",
        "        # print(target_action)\n",
        "        future_reward = self.target_critic_model.predict([new_state, target_action])[0][0]\n",
        "        reward += gamma * future_reward\n",
        "\n",
        "      self.critic_model.fit([current_state, action], reward, verbose=0) \n",
        "  \n",
        "  def train(self):\n",
        "    batch_size = 32\n",
        "    if len(memory) < batch_size:\n",
        "      return\n",
        "    rewards = []\n",
        "    samples = random.sample(self.memory, batch_size)\n",
        "    self.train_critic(samples)\n",
        "    self.train_actor(samples)\n",
        "\n",
        "  \n",
        "\n",
        "  # ------ Target Model Updating\n",
        "\n",
        "  def update_actor_target(self):\n",
        "    actor_model_weights  = self.actor_model.get_weights()\n",
        "    actor_target_weights = self.target_critic_model.get_weights()\n",
        "  \n",
        "    for i in range(len(actor_target_weights)):\n",
        "      actor_target_weights[i] = actor_model_weights[i]\n",
        "    \n",
        "    self.target_critic_model.set_weights(actor_target_weights)\n",
        "  \n",
        "  def update_critic_target(self):\n",
        "    critic_model_weights  = self.critic_model.get_weights()\n",
        "    critic_target_weights = self.target_critic_model.get_weights()\n",
        "    for i in range(len(critic_target_weights)):\n",
        "      critic_target_weights[i] = critic_model_weights[i]\n",
        "    \n",
        "    self.target_critic_model.set_weights(critic_target_weights)\n",
        "\n",
        "  \n",
        "  def update_target(self):\n",
        "    self.update_actor_target()\n",
        "    self.update_critic_target()\n",
        "  \n",
        "  # ------ Model Prediction\n",
        "\n",
        "  def act(self, current_state):\n",
        "   \n",
        "    self.epsilon *= self.epsilon_decay\n",
        "  \n",
        "    if np.random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()\n",
        "  \n",
        "    return self.actor_model.predict(current_state)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "UQAhki4nFBQb",
        "outputId": "62de191a-6f4d-4c34-80dc-51d052d6263a"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cfcdc3307126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-cfcdc3307126>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pendulum-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mactor_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor_Critic_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnum_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-2265f33d7e2e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, sess)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Actor Model State Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_state_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Actor_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Actor_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_Actor_Model() takes 0 positional arguments but 1 was given"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "\tsess = tf.compat.v1.Session()\n",
        "\tK.set_session(sess)\n",
        "\tenv = gym.make(\"Pendulum-v0\")\n",
        "\tactor_critic = Actor_Critic_Model(env, sess)\n",
        "\n",
        "\tnum_trials = 10000\n",
        "\ttrial_len  = 500\n",
        "\n",
        "\tcur_state = env.reset()\n",
        "\taction = env.action_space.sample()\n",
        " \n",
        "\twhile True:\n",
        "\t\t# env.render()\n",
        "\t\tcur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
        "\t\taction = actor_critic.act(cur_state)\n",
        "\t\taction = action.reshape((1, env.action_space.shape[0]))\n",
        "\n",
        "\t\tnew_state, reward, done, _ = env.step(action)\n",
        "\t\tnew_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
        "\n",
        "\t\tactor_critic.remember(cur_state, action, reward, new_state, done)\n",
        "\t\tactor_critic.train()\n",
        "\n",
        "\t\tcur_state = new_state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0nBHFWYnEDI"
      },
      "source": [
        "# Creating the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptFW8qY4RcBl"
      },
      "outputs": [],
      "source": [
        "def create_Actor_Model():\n",
        "\n",
        "  state_input = Input(shape = env.observation_space.shape)\n",
        "  h1 = Dense(24, activation = 'relu')(state_input)\n",
        "  h2 = Dense(24, activation = 'relu')(h1)\n",
        "  h3 = Dense(24, activation = 'relu')(h2)\n",
        "  output = Dense(env.action_space.shape[0], activation='relu')(h3)\n",
        "\n",
        "  actor = Model(inputs = state_input, outputs=output)\n",
        "  adam = tf.optimizers.Adam(lr = 0.001)\n",
        "  \n",
        "  actor.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "  return state_input, actor\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC_x9o0Q2i29"
      },
      "outputs": [],
      "source": [
        "def create_Critic_Model():\n",
        "\n",
        "  \"\"\"\n",
        "  Critic Model takes both as input the state environment and the action space and calculate a corresponding valuation\n",
        "  We do this by a series of fully-connected layers, with a layer in the middle that merges the two before combining into the final Q-value prediction\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  state_input = Input(shape = env.observation_space.shape)\n",
        "  state_h1 = Dense(24, activation = 'relu')(state_input)\n",
        "  state_h2 = Dense(48)(state_h1)\n",
        "  \n",
        "  action_input = Input(shape = env.observation_space.shape)\n",
        "  action_h1 = Dense(48)(action_input)\n",
        "\n",
        "  critic_input    = Add()([state_h2, action_h1])\n",
        "  h1 = Dense(24, activation = 'relu')(critic_input)\n",
        "\n",
        "  output = Dense(1, activation='relu')(h1)\n",
        "  \n",
        "  critic = Model(inputs = [state_input, action_input], outputs = output)\n",
        "\n",
        "  adam = tf.optimizers.Adam(lr = 0.001)\n",
        "  \n",
        "  critic.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "  return state_input, action_input, critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUTJ05CEoNLY"
      },
      "source": [
        "# Calling the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMf2VuRmFxCp"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO8DWW510jb4",
        "outputId": "74d2cab7-f57e-469f-b2ac-92af44830319"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "memory = deque(maxlen=2000)\n",
        "actor_state_input, actor_model = create_Actor_Model()\n",
        "_, target_actor_model = create_Actor_Model()\n",
        "  \n",
        "    \n",
        "actor_critic_grad = tf.compat.v1.placeholder(tf.float32, [None, env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
        "\t\t\n",
        "actor_model_weights = actor_model.trainable_weights\n",
        "actor_grads = tf.gradients(actor_model.output, actor_model_weights, - actor_critic_grad) # dC/dA (from actor)\n",
        "\n",
        "grads = zip(actor_grads, actor_model_weights)\n",
        "\n",
        "optimize = tf.keras.optimizers.RMSprop(learning_rate).apply_gradients(grads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GucEbpUXhAT2",
        "outputId": "48a2a5bf-e66d-47f4-e0cd-430102436cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        }
      ],
      "source": [
        "critic_state_input, critic_action_input, critic_model = create_Critic_Model()\n",
        "_, _, target_critic_model = create_Critic_Model()\n",
        "\n",
        "critic_grads = tf.gradients(critic_model.output, critic_action_input) # where we calcaulte de/dC for feeding above\n",
        "\t\t\n",
        "# Initialize for later gradient calculations\n",
        "sess.run(tf.compat.v1.initialize_all_variables())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5oCb2MoZ-Cg"
      },
      "source": [
        "# Training the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHvYfUnMlzTx"
      },
      "outputs": [],
      "source": [
        "def remember(cur_state, action, reward, new_state, done):\n",
        "  memory.append([cur_state, action, reward, new_state, done])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp93ry8tPkgI"
      },
      "outputs": [],
      "source": [
        "def remember(cur_state, action, reward, new_state, done):\n",
        "  memory.append([cur_state, action, reward, new_state, done])\n",
        "\n",
        "def train_actor(samples):\n",
        "\t\tfor sample in samples:\n",
        "\t\t\tcurrent_state, action, reward, new_state, _ = sample\n",
        "\t\t\tpredicted_action = actor_model.predict(current_state)\n",
        "\t\t\tgrads = sess.run(critic_grads, \n",
        "                    feed_dict={ critic_state_input:  current_state, \n",
        "                               critic_action_input: predicted_action})[0]\n",
        "\n",
        "\t\t\tsess.run(optimize, \n",
        "            feed_dict={ actor_state_input: current_state, \n",
        "                       actor_critic_grad: grads})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6ceW0xAhv6Z"
      },
      "outputs": [],
      "source": [
        "def train_critic(samples):\n",
        "  for sample in samples:\n",
        "    current_state, action, reward, new_state, done = sample\n",
        "    print(current_state)\n",
        "    print(new_state)\n",
        "\n",
        "    if not done:\n",
        "      \n",
        "      target_action = target_actor_model.predict(new_state)\n",
        "      print(target_action)\n",
        "      future_reward = target_critic_model.predict([new_state, target_action])[0][0]\n",
        "      reward += gamma * future_reward\n",
        "      \n",
        "    #\n",
        "    critic_model.fit([current_state, action], reward, verbose=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFCvjOFsjRnV"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  batch_size = 32\n",
        "  if len(memory) < batch_size:\n",
        "    return\n",
        "    \n",
        "  rewards = []\n",
        "  samples = random.sample(memory, batch_size)\n",
        "  train_critic(samples)\n",
        "  train_actor(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYYIKpuMkm8t"
      },
      "outputs": [],
      "source": [
        "# ------ Target Model Updating\n",
        "\n",
        "def update_actor_target():\n",
        "  actor_model_weights  = actor_model.get_weights()\n",
        "  actor_target_weights = target_critic_model.get_weights()\n",
        "  \n",
        "  for i in range(len(actor_target_weights)):\n",
        "    actor_target_weights[i] = actor_model_weights[i]\n",
        "    \n",
        "  target_critic_model.set_weights(actor_target_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XOV0Y2d3t-z"
      },
      "outputs": [],
      "source": [
        "def update_critic_target():\n",
        "  critic_model_weights  = critic_model.get_weights()\n",
        "  critic_target_weights = target_critic_model.get_weights()\n",
        "  for i in range(len(critic_target_weights)):\n",
        "    critic_target_weights[i] = critic_model_weights[i]\n",
        "  \n",
        "  target_critic_model.set_weights(critic_target_weights)\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZBGXwmJ3uVe"
      },
      "outputs": [],
      "source": [
        "def update_target():\n",
        "  update_actor_target()\n",
        "  update_critic_target()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5Z18R1elPPm"
      },
      "outputs": [],
      "source": [
        "# ------ Model Prediction\n",
        "\n",
        "def act(current_state):\n",
        "  global epsilon\n",
        "  epsilon *= epsilon_decay\n",
        "  \n",
        "  if np.random.random() < epsilon:\n",
        "    return env.action_space.sample()\n",
        "  \n",
        "  return actor_model.predict(current_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VgdhAnLVld9i",
        "outputId": "a4468159-46f3-448c-bdc2-87a06cd7c67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.9812367 -0.192807   7.655375 ]]\n",
            "[[-0.83963215 -0.54315543  7.6033735 ]]\n",
            "[[0.]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-68d6a1b533ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-40a4d50a2523>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-a0d61ddebabc>\u001b[0m in \u001b[0;36mtrain_critic\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtarget_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_actor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mfuture_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_critic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfuture_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m--> 699\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m    700\u001b[0m     return predict_loop(\n\u001b[1;32m    701\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2339\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2366\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    643\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    646\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_6 to have shape (3,) but got array with shape (1,)"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "learning_rate = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_decay = .995\n",
        "gamma = .95\n",
        "tau   = .125\n",
        "\n",
        "num_trials = 10000\n",
        "trial_len  = 500\n",
        "\n",
        "cur_state = env.reset()\n",
        "action = env.action_space.sample()\n",
        "while True:\n",
        "  cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
        "  action = act(cur_state)\n",
        "  action = action.reshape((1, env.action_space.shape[0]))\n",
        "  \n",
        "  new_state, reward, done, _ = env.step(action)\n",
        "  new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
        "  \n",
        "  remember(cur_state, action, reward, new_state, done)\n",
        "  train()\n",
        "  \n",
        "  cur_state = new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_mLg1L9mDWs",
        "outputId": "8d2aa916-53c0-46d7-9192-b839fa0eb463"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "def create_Actor_Model():\n",
        "\n",
        "  state_input = Input(shape = env.observation_space.shape)\n",
        "  h1 = Dense(24, activation = 'relu')(state_input)\n",
        "  h2 = Dense(24, activation = 'relu')(h1)\n",
        "  output = Dense(env.action_space.shape[0], activation='softmax')(h2)\n",
        "\n",
        "  actor = Model(inputs = [state_input], outputs=output)\n",
        "  adam = tf.optimizers.Adam(lr = 0.001)\n",
        "  \n",
        "  actor.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "  return state_input, actor\n",
        "\n",
        "\n",
        "def create_Critic_Model():\n",
        "\n",
        "  \"\"\"\n",
        "  Critic Model takes both as input the state environment and the action space and calculate a corresponding valuation\n",
        "  We do this by a series of fully-connected layers, with a layer in the middle that merges the two before combining into the final Q-value prediction\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  state_input = Input(shape = env.observation_space.shape)\n",
        "  state_h1 = Dense(24, activation = 'relu')(state_input)\n",
        "  state_h2 = Dense(48, activation = 'relu')(state_h1)\n",
        "  \n",
        "  action_input = Input(shape = env.observation_space.shape)\n",
        "  action_h1 = Dense(48, activation = 'relu')(state_input)\n",
        "\n",
        "  critic_input    = Add()([state_h2, action_h1])\n",
        "  h1 = Dense(24, activation = 'relu')(critic_input)\n",
        "\n",
        "  output = Dense(1, activation='relu')(h1)\n",
        "  \n",
        "  critic = Model(inputs = [state_input, action_input], outputs = output)\n",
        "\n",
        "  adam = tf.optimizers.Adam(lr = 0.001)\n",
        "  \n",
        "  critic.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "  return state_input, action_input, critic\n",
        "memory = deque(maxlen=2000)\n",
        "actor_state_input, actor_model = create_Actor_Model()\n",
        "_ , target_actor_model = create_Actor_Model()\n",
        "\n",
        "actor_critic_grad = tf.compat.v1.placeholder(tf.float32,  [None, env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
        "\n",
        "actor_model_weights = actor_model.trainable_weights\n",
        "\n",
        "actor_grads = tf.gradients(actor_model.output, actor_model_weights, - actor_critic_grad) # dC/dA (from actor)\n",
        "grads = zip(actor_grads, actor_model_weights)\n",
        "optimize = tf.keras.optimizers.RMSprop(learning_rate).apply_gradients(grads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "FkuBuOjMGdYb",
        "outputId": "452fb060-7757-48ed-c881-abe901888cf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c467eccb0a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-c467eccb0a97>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mtrain_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mtrain_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# ------ Target Model Updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-c467eccb0a97>\u001b[0m in \u001b[0;36mtrain_critic\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mtarget_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_actor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mfuture_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_critic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfuture_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m--> 699\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m    700\u001b[0m     return predict_loop(\n\u001b[1;32m    701\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2339\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2366\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    643\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    646\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_28 to have shape (3,) but got array with shape (1,)"
          ]
        }
      ],
      "source": [
        "critic_state_input, critic_action_input, critic_model = create_Critic_Model()\n",
        "_, _, target_critic_model = create_Critic_Model()\n",
        "\n",
        "critic_grads = tf.gradients(critic_model.output, critic_action_input) # where we calcaulte de/dC for feeding above\n",
        "\t\t\n",
        "# Initialize for later gradient calculations\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "def remember(cur_state, action, reward, new_state, done):\n",
        "  memory.append([cur_state, action, reward, new_state, done])\n",
        "\n",
        "\n",
        "def train_actor(samples):\n",
        "\t\tfor sample in samples:\n",
        "\t\t\tcurrent_state, action, reward, new_state, _ = sample\n",
        "\t\t\tpredicted_action = actor_model.predict(current_state)\n",
        "\t\t\tgrads = sess.run(critic_grads, feed_dict={ critic_state_input:  current_state, critic_action_input: predicted_action})[0]\n",
        "\n",
        "\t\t\tsess.run(optimize, feed_dict={ actor_state_input: current_state, actor_critic_grad: grads})\n",
        "\n",
        "\n",
        "def train_critic(samples):\n",
        "  for sample in samples:\n",
        "    current_state, action, reward, new_state, done = sample\n",
        "    if not done:\n",
        "      target_action = target_actor_model.predict(new_state)\n",
        "      future_reward = target_critic_model.predict([new_state, target_action])[0]\n",
        "      reward += gamma * future_reward\n",
        "      \n",
        "    #\n",
        "    critic_model.fit([current_state, action], reward, verbose=0)\n",
        "def train():\n",
        "  batch_size = 32\n",
        "  if len(memory) < batch_size:\n",
        "    return\n",
        "    \n",
        "  rewards = []\n",
        "  samples = random.sample(memory, batch_size)\n",
        "  train_critic(samples)\n",
        "  train_actor(samples)\n",
        "# ------ Target Model Updating\n",
        "\n",
        "def update_actor_target():\n",
        "  actor_model_weights  = actor_model.get_weights()\n",
        "  actor_target_weights = target_critic_model.get_weights()\n",
        "  \n",
        "  for i in range(len(actor_target_weights)):\n",
        "    actor_target_weights[i] = actor_model_weights[i]\n",
        "    target_critic_model.set_weights(actor_target_weights)\n",
        "\n",
        "def update_critic_target():\n",
        "  critic_model_weights  = critic_model.get_weights()\n",
        "  critic_target_weights = target_critic_model.get_weights()\n",
        "  for i in range(len(critic_target_weights)):\n",
        "    critic_target_weights[i] = critic_model_weights[i]\n",
        "  \n",
        "  target_critic_model.set_weights(critic_target_weights)\t\t\n",
        "\n",
        "def update_target():\n",
        "  update_actor_target()\n",
        "  update_critic_target()\n",
        "# ------ Model Prediction\n",
        "\n",
        "def act(current_state):\n",
        "  global epsilon\n",
        "  epsilon *= epsilon_decay\n",
        "  \n",
        "  if np.random.random() < epsilon:\n",
        "    return env.action_space.sample()\n",
        "  \n",
        "  return actor_model.predict(current_state)\n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "learning_rate = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_decay = .995\n",
        "gamma = .95\n",
        "tau   = .125\n",
        "\n",
        "num_trials = 10000\n",
        "trial_len  = 500\n",
        "\n",
        "cur_state = env.reset()\n",
        "action = env.action_space.sample()\n",
        "while True:\n",
        "  cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
        "  action = act(cur_state)\n",
        "  action = action.reshape((1, env.action_space.shape[0]))\n",
        "  \n",
        "  new_state, reward, done, _ = env.step(action)\n",
        "  new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
        "  \n",
        "  remember(cur_state, action, reward, new_state, done)\n",
        "  train()\n",
        "  \n",
        "  cur_state = new_state\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
